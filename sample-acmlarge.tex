%%
\documentclass[acmlarge, nonacm]{acmart}
\usepackage{graphicx}
\usepackage{hyperref}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}


\title{Exploring the Impact of an Empathy-Based Sandbox
Approach for Privacy Awareness in Mobile Ecosystems}

\author{Ibrahim Khalilov}
\email{ibrahimk@vt.edu}
\affiliation{%
  \institution{Virginia Tech}
  \state{Virginia}
  \country{USA}
}


\renewcommand{\shortauthors}{Khalilov et al.}

\maketitle

\section{Introduction}

\subsection{Problem Statement: Privacy Awareness and the Challenge of Mobile Data Collection}

Mobile applications have become an essential part of daily life, providing convenience through services like navigation, social networking, and personalized recommendations. However, these services come at a cost—continuous data collection. Apps frequently request access to GPS location, sensor readings, microphone inputs, and browsing activity, creating a complex web of data exchange that most users do not fully comprehend. While research has shown that users express concerns about their privacy, their actual behaviors tell a different story, a phenomenon known as the privacy paradox \cite{baruh2017big}. Despite stating that they value privacy, users regularly accept permissions and agree to terms without fully understanding how their data is collected, shared, and utilized \cite{kokolakis2017privacy}.

The issue is exacerbated by the interconnected nature of mobile app permissions. Unlike desktop web tracking, where privacy settings are often confined to cookies or browsing history, mobile applications have deeper access to data that continuously updates in real-time. For instance, enabling GPS for a navigation app might seem straightforward, but in reality, this access can extend to third-party services embedded within the app, such as advertising networks or analytics platforms. Similarly, microphone permissions may allow continuous background audio collection, raising concerns about unintended surveillance \cite{Gao2019}. These permissions often interact in unexpected ways, amplifying privacy risks that users are unaware of.

Compounding this issue, privacy policies remain lengthy, difficult to interpret, and often vague about how collected data is actually used \cite{ObarandOeldorf-Hirsch2020}. Many users accept these terms not because they agree with them but because they have no real choice—denying permissions often means losing access to essential services. Furthermore, even privacy-conscious users struggle to predict the real-world consequences of their privacy choices, leading to uninformed consent.

\subsection{The Role of Sensor Data and Real-Time Privacy Risks}

As mobile applications evolve, real-time sensor data tracking has become increasingly pervasive. Beyond traditional tracking methods like browsing history and IP addresses, modern apps leverage a combination of location, accelerometer, gyroscope, ambient light, battery status, and other sensors to build detailed user profiles. These data points shape personalized user experiences, such as location-based recommendations, targeted advertising, and predictive analytics. While these features enhance usability, they also introduce significant privacy challenges related to transparency and user control \cite{rathi2025predictive}

Current privacy-enhancing tools—including incognito modes, ad blockers, and permission managers—primarily focus on static privacy protections. For instance, incognito mode prevents browsers from storing history, but it does not stop websites from tracking users via IP addresses, cookies, or fingerprinting techniques. Similarly, ad blockers can block advertisements but do not prevent apps from collecting sensor-based data in real time. These tools, while useful in certain contexts, fail to address the core issue of mobile privacy awareness: helping users understand how their data is continuously being tracked and used. The lack of a risk-free environment where users can test and visualize the impact of their privacy choices makes it difficult for them to make truly informed decisions.

\subsection{Research Questions and Proposed Solution}

To address the gap in privacy awareness, this study introduces an Empathy-Based Privacy Sandbox, a controlled, interactive environment where users can experiment with mobile app permissions, manipulate synthetic sensor data, and observe the consequences of their privacy choices in real time. Existing privacy tools, such as permission managers and system notifications, offer only static, one-time configurations without providing users with an intuitive understanding of how their data influences system behavior over time. In contrast, this sandbox allows users to actively test different privacy settings, modify sensor inputs, and see real-time system adaptations, fostering a more engaging and experiential approach to privacy awareness.

Building on the Empathy-Based Privacy Sandbox framework proposed by \citet{Chaoran2023EmpathySandbox}, this study extends its application beyond web-based environments to mobile ecosystems, where continuous data collection and real-time tracking pose unique privacy challenges. Unlike prior interventions focused on metadata manipulation, this research incorporates sensor data spoofing, advertising identifier manipulation, and behavioral activity simulation—components essential for understanding the dynamic privacy risks of mobile applications. By allowing hands-on experimentation, the sandbox enables users can manipulate these variables to better understand what kind of data can be collected and how it affects system decisions.

This study is guided by the following overarching research question: \textit{How can an interactive, sandbox-based privacy education system enhance mobile users' understanding of data permissions and improve informed decision-making?}

To answer this, we further examine:
\begin{itemize}
    \item How do users perceive privacy risks associated with real-time sensor tracking in mobile apps?
    \item How can an empathy-based sandbox improve users’ understanding of how their data footprint influences system behavior?
    \item How does hands-on experimentation with altered mobile data inputs enhance users' understanding of privacy risks and influence their decision-making confidence?
\end{itemize}

Through controlled user studies, we aim to evaluate the effectiveness of sandbox-based learning in improving privacy awareness, decision-making, and behavioral shifts. By adapting \citet{Chaoran2023EmpathySandbox}'s methodology to mobile environments, this research advances HCI scholarship on privacy education, mobile transparency, and user empowerment in sensor-driven ecosystems.


\subsection{Novelty and Contributions}

While previous research has explored privacy education through just-in-time notifications and text-based policies, these approaches have largely failed to provide an interactive, experiential learning model \cite{feng2021yaodesign}. Recent work by \citet{Chaoran2023EmpathySandbox} introduced an empathy-based privacy sandbox that allows users to explore how manipulated personal information affects system behaviors in web environments. Their study demonstrated that persona-based data experimentation enhances user awareness of privacy risks by enabling controlled manipulation of digital identities. However, their work remains limited to web-based privacy risks, such as cookie tracking, ad personalization, and metadata spoofing, without addressing the more persistent, real-time tracking challenges present in mobile applications.

This study extends \citet{Chaoran2023EmpathySandbox}'s methodology to mobile ecosystems, adapting and expanding the persona-driven experimentation framework to account for the unique privacy risks of mobile applications, where data collection is continuous, pervasive, and deeply embedded in system functionality. Unlike the web, where tracking largely relies on cookies, browsing history, and explicit user inputs, mobile platforms leverage sensor data, advertising identifiers, and cross-app behavioral patterns to infer user preferences, making privacy risks less transparent and more difficult to control.

This study makes the following key contributions to HCI and privacy research:
\begin{itemize}
    \item 

    \textbf{Extending Privacy Awareness Research to Mobile Ecosystems.}
While prior studies have explored privacy experimentation in web environments, mobile applications present unique challenges due to continuous, real-time data collection, including sensor usage, cross-app tracking, and system-level identifiers. This study expands privacy awareness research to mobile environments, equipping users with new tools to explore how real-time data streams impact app behavior, content personalization, and tracking mechanisms.
    
    \item \textbf{A Multi-Layered Data Manipulation Framework for Privacy Learning.}
Unlike previous work that primarily focused on metadata and identity spoofing, our study introduces a broader range of manipulations relevant to mobile privacy. Users can experiment not only with sensor data (e.g., GPS, motion, battery levels) but also advertising identifiers, system-level settings, and behavioral traces—providing a more comprehensive and realistic representation of how their data footprint influences mobile app ecosystems.
\end{itemize}

By bridging the gap between abstract privacy policies and real-world data interactions, this study contributes to the broader discourse on usable privacy design in HCI and offers practical design implications for future privacy-enhancing technologies.

\section{Lietrature Review 1000-1200 words}

\subsection{Privacy Paradox: Why Awareness Alone Fails to Drive Meaningful Privacy Decisions}

Despite increasing public concern over digital privacy, users routinely share personal data without fully understanding its implications. This contradiction, known as the privacy paradox, has been extensively studied \cite{Gerber2018Explaining}. Users often claim to prioritize privacy, yet they regularly accept broad and unrestricted app permissions, demonstrating an inconsistency between attitudes and behaviors \cite{baruh2017big}, \cite{barth2017privacy}.

The privacy paradox arises from several key factors that shape user behavior. One of the primary challenges is cognitive overload, as privacy policies are often excessively long, dense with legal jargon, and difficult to interpret. As a result, many users do not take the time to read or fully understand these policies, leading to uninformed decision-making \cite{Obar2018The}. Another critical factor is the lack of clear mental models regarding how personal data is collected, processed, and shared across different platforms. Without a concrete understanding of these data flows, users struggle to anticipate the real-world consequences of granting permissions, making it difficult to make informed privacy choices \cite{balebako2022nudging}. Additionally, users frequently face convenience versus privacy trade-offs, where denying permissions can result in restricted access to essential apps and services. In many cases, users accept these permissions not because they are comfortable with the privacy implications, but because refusing access would significantly limit their ability to use the app’s core functionalities \cite{Fleischhauer2022Paradox}. These combined factors create an environment where users, despite their stated privacy concerns, routinely engage in behaviors that expose them to greater data collection risks.

While existing privacy awareness efforts focus on educating users via policies, prompts, and notifications, these methods have largely failed to change behavior because they are passive, abstract, and easily ignored \cite{feng2021yaodesign}. This highlights the need for more engaging, interactive approaches that allow users to experience privacy risks in real time.

\subsection{Mobile-Specific Privacy Challenges}
The privacy landscape for mobile applications is distinct from that of traditional web-based environments, as mobile platforms leverage real-time sensor data to track users continuously. Unlike web tracking, which primarily relies on cookies and metadata, mobile applications integrate various built-in sensors—such as GPS, accelerometers, gyroscopes, microphones, and ambient light detectors—to monitor user behavior. This real-time data collection poses complex privacy challenges, as it allows for uninterrupted tracking and inference of user activities, often without explicit user awareness \cite{CAHAR}.

One of the key concerns is location tracking, which extends beyond navigation purposes. Studies have shown that many mobile apps persistently collect location data even when the app is not in use, often sharing it with third-party advertisers and data brokers \cite{bian2021supply}. Similarly, microphone access raises significant privacy concerns, as applications can process ambient audio for targeted advertising or user profiling without clear disclosure \cite{pudasaini2024comprehensive}. Motion sensors and accelerometer data, which were initially designed for fitness tracking and device interactions, have been found to infer users’ routines, detect transportation modes, and even estimate emotional states \cite{niemeijer2023promise}. Furthermore, ambient light sensors and proximity detectors, typically used for screen adjustments, have been exploited for device fingerprinting and covert tracking \cite{berdich2023survey}.

These privacy threats are exacerbated by mobile security vulnerabilities. Unlike permissions-based access control mechanisms, research indicates that many mobile applications bypass explicit user consent to access sensor data, utilizing indirect inference techniques \cite{aburas2024user}. This makes sensor-based tracking not only a privacy concern but also a security risk, as attackers can manipulate these sensors to extract sensitive behavioral patterns. Given the passive, continuous, and often hidden nature of mobile data collection, traditional privacy tools—such as permission managers and privacy dashboards—fail to provide users with a clear and practical understanding of real-time data exposure.

To bridge this gap, this study introduces an Empathy-Based Privacy Sandbox, enabling users to experiment with synthetic sensor data in a controlled environment. By allowing users to manipulate their GPS, motion, and metadata inputs, the sandbox provides firsthand experience of mobile tracking mechanisms, fostering interactive privacy education. This experiential approach aims to shift privacy literacy from passive awareness to active decision-making, equipping users with a deeper understanding of how their sensor data is collected and utilized in mobile ecosystems.

\subsection{Existing Approaches to Privacy Awareness \& Their Limitations}

\begin{table*}[ht]
\centering
\begin{tabular}{|p{2cm}|p{3cm}|p{10cm}|}
\hline
\textbf{Category} & \textbf{References} & \textbf{Limitations} \\
\hline
\textbf{Privacy Policies} & Döbelt \& Halama (2023); Panadés \& Yuguero (2025); Obar \& Oeldorf-Hirsch (2020) & Legally mandated disclosures that outline how user data is collected and processed. Often ignored due to length and complexity; users rarely read them, leading to uninformed consent. \\
\hline
\textbf{Just-in-Time Permission Prompts} & Acquisti et al. (2023); Femmam (2024); Wash (2010) & Appear at the moment of decision, asking users to grant or deny access to data. Users frequently dismiss them without reading; decision fatigue leads to automatic acceptance. \\
\hline
\textbf{Privacy Dashboards} & Wilson et al. (2024); Parti et al. (2024); Panadés \& Yuguero (2025) & Aggregate data access history and provide users with control options. Lack real-time feedback and proactive alerts; overly technical for non-expert users, limiting effectiveness. \\
\hline
\textbf{Ad Blockers \& Tracker Prevention} & Sikder et al. (2017); Guo et al. (2021); Rangasamy et al. (2024) & Designed to block online trackers and ad-based profiling. Effective for web tracking but fail to prevent mobile sensor-based tracking, which occurs through system-level interactions. \\
\hline
\textbf{Gamified Privacy Education} & Bahrini et al. (2024); Shah \& Agarwal (2023); Giannakas et al. (2023) & Engaging and interactive learning methods that use games or challenges to educate users on privacy risks. Lack real-world applicability; effectiveness varies based on user engagement and prior knowledge. \\
\hline
\textbf{Empathy-Based Sandbox for Privacy Learning} & Chen et al. (2024); Menard \& Bott (2025); Guo et al. (2021) & Allows users to manipulate digital personas to explore privacy risks in a controlled environment. Limited to web-based environments; not yet adapted for mobile tracking mechanisms where real-time data flows persist. \\
\hline
\end{tabular}
\caption{Grouped Literature Review on Privacy Awareness Tools and Their Limitations in Mobile Environments}
\label{tab:privacy_awareness_lit_review}
\end{table*}

Several methods have been developed to improve privacy awareness, yet most do not drive significant behavioral change (Table \ref{tab:privacy_awareness_lit_review}). Privacy policies, despite being legally mandated, are often too lengthy and complex for users to engage with meaningfully, leading to uninformed consent \cite{burkhardt2023privacy}. Just-in-time permission prompts, while intended to enhance user control, frequently result in decision fatigue, causing users to accept permissions without considering the consequences \cite{bilogrevic2021shhh}. Similarly, privacy dashboards, which aggregate data access history, lack real-time tracking insights, limiting their usefulness in preventing unwanted data collection \cite{narayanan2024real}.

Beyond these limitations, ad blockers and tracker prevention tools have proven effective in restricting web-based tracking but are ineffective against sensor-based data collection on mobile devices, which occurs at the system level \cite{bian2021supply}. Gamified privacy education approaches have shown promise in improving user engagement with privacy concepts but often fall short in fostering real-world behavioral changes \cite{idierukevbe2024bridging}. Although sandbox-based privacy education has been successfully applied to web environments to help users explore tracking risks, these models have yet to be adapted for mobile ecosystems, where real-time sensor data collection presents distinct challenges \cite{Chaoran2023EmpathySandbox}. Given these shortcomings, there is a clear need for an interactive, real-time learning approach that allows users to actively experiment with mobile privacy settings and understand how their data is being collected and processed.

\subsection{Empathy-Based Sandboxes in Privacy Research}
As discussed in Section 2.3, existing privacy awareness tools struggle to provide users with a clear, actionable understanding of mobile tracking mechanisms. In response, sandbox-based interventions have gained traction in privacy research, offering users interactive environments where they can experiment with privacy settings and observe system responses firsthand. Unlike static privacy dashboards or permission managers, which rely on passive information delivery, sandboxes promote experiential learning, allowing users to engage with privacy risks dynamically (Chen et al., 2024). The studies summarized in Table \ref{tab:privacy_awareness_lit_review} highlight how sandbox models have been applied in web-based privacy education, yet they have not been adapted to mobile ecosystems, where real-time sensor tracking presents distinct challenges. Most prior sandbox-based interventions have focused on web environments, particularly metadata manipulation like browser attributes and cookie tracking (Menard \& Bott, 2025). However, these methods do not account for mobile-specific tracking mechanisms, where apps continuously collect GPS, motion, and sensor data in the background (Giannakas et al., 2023). However, these models do not address the unique challenges of real-time, sensor-driven tracking in mobile apps.

\subsection{Sensor Data and Mobile Privacy Challenges: REMOVE IF NO SPACE}
 ---> WE WILL SEE IF WE HAVE SPACE SPECIFICALLY FOR THIS

\section{Proposed Research Design and Methodology (1000-1400 words)}

\subsection{Empathy-Based Privacy Sandbox for Mobile}

\subsection{Study Design: Experimental Setup}

\subsection{Study Procedure}

\section{Planned Evaluation and Expected Outcomes (800 words)}

\subsection{Expected Imact on Privacy Awarewness}

\subsection{Anticipated User Reactions}

\subsection{Behavioral Adjustments Post-Experiment}

\subsection{Comparison to Traditional Privacy Tools}

\section{Discussion and Broader Implications (800 words)}

\subsection{Contributions to HCI and Privacy Research}

\subsection{Implications for Mobile App Design and Policy}

\subsection{Challenges and Potential Limitations}

\subsection{Future Research Directions}

\section{Conclusion (400 words)}


















\section{OLD Version:  Just for reference}

Mobile ecosystems introduce new dimensions of privacy concerns due to the \textit{always-on nature} of sensor data tracking. Applications increasingly use sensor-based inferences for health monitoring, location tracking, motion-based authentication, and behavioral profiling. Unlike traditional web tracking, mobile data collection is often \textit{passive, continuous, and harder to opt out of}. There is limited work on \textit{how users perceive and understand mobile sensor tracking}, making this an essential area of study.

While previous studies have framed the privacy paradox primarily in the context of web browsing, metadata, and social media, mobile systems introduce an additional layer of complexity. Unlike traditional platforms where privacy is largely managed through account settings and permission toggles, mobile applications continuously collect sensor-driven data such as location, motion patterns, ambient conditions, and biometric inputs. These sensor readings provide granular, real-time behavioral insights, making it even more difficult for users to understand how their data is being used.

The continuous and passive nature of sensor data collection amplifies the privacy attitudes-behavior gap. Research in contextual integrity (Nissenbaum, 2019) highlights that users’ privacy expectations are often misaligned with actual data practices—they may accept GPS tracking for navigation but fail to realize that the same location data is being used for targeted advertising, behavioral profiling, and third-party sharing. Unlike web-based tracking, where cookies can be deleted or blocked, sensor data collection operates at the system level, often without clear opt-out mechanisms.

Moreover, research has shown that mobile users make privacy decisions reactively rather than proactively. Unlike traditional web privacy settings, where users configure preferences in advance, mobile data-sharing decisions occur in-the-moment, often as one-time permission prompts (Vitale et al., 2018). This further contributes to inconsistent privacy behaviors, as decisions are made without a full understanding of long-term implications.

\subsection{Bridging the Privacy Awareness Gap Through Experimentation}
One proposed method to address this gap is to provide risk-free environments where users can explore and experience privacy consequences firsthand. Traditional privacy education relies on informational approaches, such as warning labels and tutorials, which have been shown to be only moderately effective in changing behavior (Wang et al., 2021). However, experiential learning theories suggest that users are more likely to internalize privacy concepts when they engage directly with system outcomes.

Previous research, such as the work of Chaoran et. al. \cite{Chaoran2023EmpathySandbox}, has demonstrated that sandboxed environments—where users can test different privacy settings and observe system responses—lead to greater privacy literacy and more consistent decision-making. However, these studies have primarily focused on web-based environments, where users manipulated browser metadata, search history, and cookie tracking. Our research extends this concept into mobile sensor ecosystems, which requires additional layers of sensor spoofing, real-time data manipulation, and controlled experimental conditions.

This transition from web-based privacy experiments to sensor-driven mobile environments necessitates new methodologies that account for:

The continuous and real-time nature of sensor data collection, which differs from static privacy settings in browsers.
The integration of multiple sensors (GPS, motion, battery, environmental factors) into system decision-making, making privacy consequences harder to trace.
The complexity of app ecosystems, where system-wide data flows (e.g., fused location tracking, background permissions) operate beyond a single app’s interface.
This study builds upon sandbox-based privacy frameworks but introduces sensor spoofing as an additional experimental mechanism, allowing users to see, manipulate, and reflect on the ways mobile applications interpret their data. The following section details the theoretical foundations of empathy-based sandboxes and their role in privacy research.



\section{Proposed Framework}

\subsection{Design Objectives}
The proposed system will:
\begin{enumerate}
    \item Provide a risk-free environment where users can explore privacy settings and experiment with sensor data manipulation.
    \item Enable users to \textit{observe the relationship between their data and system outcomes} through controlled experimentation.
    \item Expand the \textit{empathy-based privacy sandbox model} to integrate real-time \textit{sensor spoofing} (e.g., GPS, accelerometer, gyroscope, light sensor, battery data) in addition to traditional metadata manipulation.
    \item Investigate the impact of manipulated sensor data on user privacy decisions, user perceptions of risk, and behavioral changes regarding data-sharing practices.
\end{enumerate}

\subsection{Components of the Sandbox}
\begin{itemize}
    \item \textbf{Synthetic Persona Generation}: Users define synthetic personas with demographic and behavioral patterns. These personas interact with applications under \textit{artificially generated} data configurations.
    \item \textbf{Sensor and Data Spoofing Module}: Includes \textit{spoofing capabilities} for GPS, accelerometer, gyroscope, and additional mobile sensors. Users can manipulate sensor input data to observe changes in app behavior (e.g., a \textit{spoofed low battery level} might trigger app optimizations or recommendations).
    \item \textbf{Interactive Feedback Mechanism}: Users interact with real apps and \textit{observe system responses} to their synthetic sensor data. A visualization tool highlights \textit{how manipulated data alters personalization, recommendations, and app functionalities}.
    \item \textbf{Privacy Awareness and Literacy Tools}: Users receive \textit{real-time explanations} of how their sensor data is being used. Post-experiment reflections help users connect \textit{privacy decisions} to actual system outcomes.
\end{itemize}

\section{Study Design}

\subsection{Research Questions}
\begin{itemize}
    \item How do users perceive privacy risks associated with real-time sensor tracking in mobile apps?
    \item How can an empathy-based sandbox improve users’ understanding of how their data footprint influences system behavior?
    \item How does hands-on experimentation with sensor data lead to increased \textit{privacy awareness and decision-making confidence}?
\end{itemize}

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Participants}: 30--50 participants from diverse demographic backgrounds.
    \item \textbf{Tasks}: Participants interact with mobile apps using \textit{synthetic sensor inputs}, logging observations and reflections.
    \item \textbf{Scenarios}:
        \begin{itemize}
            \item \textbf{GPS Spoofing}: Testing how location-based recommendations shift when locations are modified.
            \item \textbf{Motion Data Alteration}: Observing changes in health app recommendations when motion patterns are adjusted.
            \item \textbf{Battery Spoofing}: Testing app optimizations when battery levels are artificially set to low or high.
        \end{itemize}
\end{itemize}

\section{Anticipated Results and Contributions}

\subsection{Expected Outcomes}
\begin{itemize}
    \item Users will gain \textit{firsthand experience} in privacy decision-making, improving awareness and literacy.
    \item App behavior will demonstrate \textit{measurable shifts} based on manipulated sensor inputs.
    \item The study will inform \textit{policy recommendations} for designing \textit{more transparent privacy settings} in mobile apps.
\end{itemize}

\subsection{Broader Impacts}
\begin{itemize}
    \item Findings can \textit{guide regulators} in framing user-friendly privacy policies.
    \item Insights will help \textit{developers design more transparent and user-controllable privacy settings}.
\end{itemize}


\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
\endinput
%%
%% End of file `sample-acmlarge.tex'.
